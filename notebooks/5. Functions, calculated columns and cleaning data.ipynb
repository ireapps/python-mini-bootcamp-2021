{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions, calculated columns and cleaning data\n",
    "\n",
    "In this notebook, we'll cover writing custom functions, adding calculated columns and a few data-cleaning strategies.\n",
    "\n",
    "First, import pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "If you find yourself doing the same thing over and over again in your code, it might be time to write a function.\n",
    "\n",
    "Functions are blocks of reusable code -- little boxes that (usually) take inputs and (usually) return outputs. In Excel, `=SUM()` is a function. `print()` is one of Python's built-in function.\n",
    "\n",
    "You can also _define your own functions_. This can save you some typing, and it will help separate your code into logical, easy-to-read pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax\n",
    "\n",
    "Functions start with the `def` keyword -- short for _define_, because you're defining a function -- then the name of the function, then parentheses (sometimes with the names of any `arguments` your function requires inside the parentheses) and then a colon. The function's code sits inside an indented block immediately below that line. In most cases, a function will `return` a value at the end.\n",
    "\n",
    "Here is a function that takes a number and returns that number multiplied by 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_ten(number):\n",
    "    return number * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `number` argument is just a placeholder for whatever value is handed the function as an input. We could have called that argument `banana` and things would be just fine (though it would be confusing for people reading your code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling a function\n",
    "\n",
    "By itself, a function doesn't do anything. We have built a tiny machine to multiply a number by 10. But it's just sitting on the workshop bench, waiting for us to use it.\n",
    "\n",
    "Let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_ten(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function arguments\n",
    "\n",
    "Functions can accept _positional_ arguments or _keyword_ arguments.\n",
    "\n",
    "If your function uses _positional_ arguments, the order in which you pass arguments to the function matters. Here is a function that prints out a message based on its input: a person's name and their hometown.\n",
    "\n",
    "(This function uses something called an \"f-string\" to format the result. For more information on text formatting, see [this notebook](String%20formatting.ipynb).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name, hometown):\n",
    "    return f'Hello, {name} from {hometown}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet('Cody', 'Pavillion, WY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the order of the arguments, we get nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet('Pavillion, WY', 'Cody')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using _keyword_ arguments requires us to specify what value belongs to what argument, and it allows us to set a default value for the argument -- values that the function will use if you fail to pass any arguments when you call it. We could rewrite our function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name='Cody', hometown='Pavillion, WY'):\n",
    "    return f'Hello, {name} from {hometown}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it doesn't matter what order we pass in the arguments, because we're defining the keyword that they belong to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet(hometown='Pittsburgh, PA', name='Jacob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we call the `greet()` function without any arguments at all, now? It'll use the default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Try it yourself\n",
    "\n",
    "Use the code blocks below to experiment with functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new or calculated columns\n",
    "\n",
    "In a spreadsheet program, if you want to add a new column of data -- maybe a copy of an existing column for cleaning -- you could just reference the original column in a formula. If you wanted to calculate a new column of values based on other values in each row, you might write a formula and fill it down. In SQL, you might run an `ALTER TABLE`/`UPDATE`/`SET` routine to handle this process.\n",
    "\n",
    "In pandas, adding a new column is similar to adding a new record to a Python dictionary. Let's load in the CT overdose data to take a look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct = pd.read_excel('../data/CT_Overdoses_2012-2016.xlsx', sheet_name='Accidental_Drug_Related_Deaths_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we eventually wanted to do some analysis based on the `Death City` column, but maybe first we need to clean it up. You always want to leave your original data intact, so first step would be to create a copy of the `Death City` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct['death_city_clean'] = df_ct['Death City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then you could work through some cleaning steps (more on that below).\n",
    "\n",
    "To create a calculated column, you would first define a function to process a row of data in your dataframe, then _apply_ that function to your dataframe using a pandas method called [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html).\n",
    "\n",
    "The values in several columns in our dataframe list whether a particular drug was found by the medical examiner examining the body, with `Y` meaning it was found and the default pandas null value (`NaN`) if not. Let's add a new column, `drugs_involved_total`, that totals up the number of `Y`s in each row for the columns listing individual drugs:\n",
    "\n",
    "```python\n",
    "'Heroin',\n",
    "'Cocaine',\n",
    "'Fentanyl',\n",
    "'Oxycodone',\n",
    "'Oxymorphone',\n",
    "'EtOH',\n",
    "'Hydro-codeine',\n",
    "'Benzodiazepine',\n",
    "'Methadone',\n",
    "'Amphet',\n",
    "'Tramad',\n",
    "'Morphine (not heroin)',\n",
    "'Other'\n",
    "```\n",
    "\n",
    "Now we can write a function that accepts as its one position argument a row of data in the dataframe, checks the values in each of our target columns -- keeping track of the `Y`s -- and then returns the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the function is more or less arbitrary\n",
    "# `row` also an arbitrary argument name but helps us think about what's happening\n",
    "def get_total_drugs(row):\n",
    "\n",
    "    # start a counter for how many drugs were present\n",
    "    total_drugs = 0\n",
    "    \n",
    "    # list the names of the columns to check\n",
    "    drug_columns = [\n",
    "        'Heroin',\n",
    "        'Cocaine',\n",
    "        'Fentanyl',\n",
    "        'Oxycodone',\n",
    "        'Oxymorphone',\n",
    "        'EtOH',\n",
    "        'Hydro-codeine',\n",
    "        'Benzodiazepine',\n",
    "        'Methadone',\n",
    "        'Amphet',\n",
    "        'Tramad',\n",
    "        'Morphine (not heroin)',\n",
    "        'Other'        \n",
    "    ]\n",
    "    \n",
    "    # loop over the column list\n",
    "    for col in drug_columns:\n",
    "        \n",
    "        # grab the value for that column in this row\n",
    "        value = row[col]\n",
    "        \n",
    "        # if the value is `Y` ...\n",
    "        if value == 'Y':\n",
    "            \n",
    "            # ... increment the counter \n",
    "            # (this is just a shortcut for `total_drugs = total_drugs + 1`)\n",
    "            total_drugs += 1\n",
    "    \n",
    "    # once the loop completes, return the counter\n",
    "    return total_drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a function defined, you can `apply()` it to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct['drugs_involved_total'] = df_ct.apply(get_total_drugs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct.drugs_involved_total.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 For more information on applying functions to a pandas data frame, [check out this notebook](Using%20the%20apply%20method%20in%20pandas.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "\n",
    "For cleaning jobs of any size, specialized tools like [OpenRefine](http://openrefine.org/) are still your best bet -- a typical workflow is to clean your data in OpenRefine, export as a CSV, then load into pandas.\n",
    "\n",
    "But in many cases, you can use some of pandas' built-in tools to whip your data into shape. This is especially useful for data processing tasks that you plan to repeat as the data are updated.\n",
    "\n",
    "In Excel, running a pivot table (with counts) for each column will show you misspellings, external white space, inconsistent casing and other problems that keep your data from grouping correctly.\n",
    "\n",
    "In SQL, you might do the same thing with The Golden Query™️:\n",
    "\n",
    "```sql\n",
    "SELECT column, COUNT(*)\n",
    "FROM table\n",
    "GROUP BY column\n",
    "ORDER BY 2 DESC\n",
    "```\n",
    "\n",
    "To do the equivalent operation in pandas, you can just call the `value_counts()` method on a column. Let's look at some Congressional junkets data as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets = pd.read_csv('../data/congress_junkets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `value_counts()` on the _Destination_ colummn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default sort order is by count descending, but it can also be helpful in finding typos to sort by the name -- the \"index\" of what `value_counts()` returns. To do that, tack on `sort_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets['Destination'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now we start to see some common data problems in our 838 unique destinations -- whitespace, inconsistent values for the same thing (\"Accra\" and \"Accra, Ghana\") -- and can start fixing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing whitespace, casing and other \"string\" problems\n",
    "\n",
    "If part of our analysis hinged on having a pristine \"Destination\" column, then we've got some work ahead of us. First thing I'd do: Strip whitespace and upcase the text.\n",
    "\n",
    "You can do a lot of basic cleanup like this by applying Python's built-in string methods to the `str` attribute of a column.\n",
    "\n",
    "To start with, let's create a new column, `destination_clean`, with a stripped/uppercase version of the destination data.\n",
    "\n",
    "**Note**: Outside of pandas, you can use \"method chaining\" to apply multiple transformations to a string, like this: `'   My String'.upper().strip()`.\n",
    "\n",
    "When you're chaining string methods on the `str` attribute of a pandas column series, though, it doesn't work like that -- you have to call `str` after each method call. In other words:\n",
    "\n",
    "```python\n",
    "# this will throw an error\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().strip()\n",
    "\n",
    "# this will work\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets['destination_clean'] = df_junkets['Destination'].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `value_counts()` again to see if that helped at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets['destination_clean'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That eliminated a handful of problems. Now comes the tedious work of identifying entries to find and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk-replacing values with other values\n",
    "\n",
    "If we were at this point in Excel, we'd scroll through the list of unique names and start making notes of what we need to change. Same story here.\n",
    "\n",
    "Let's loop over a [sorted](https://docs.python.org/3/howto/sorting.html) list of `unique()` destinations and `print()` each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for destination in sorted(df_junkets.destination_clean.unique()):\n",
    "    print(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is where we're going to start encoding our editorial choices. \"Ames, IA\" or \"Ames, Iowa\"? \"Baku, Azerjaijan,\" or \"Baku, Republic of Azerbaijan\"? Etc.\n",
    "\n",
    "There are several ways we could structure this data, but a dictionary makes some sense based on what we need to do, so let's do that. Each key will be a string that we'd like to replace; each value will be the string we'd like to replace it with. To get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_fixes = {\n",
    "    'BAKU, AZERBIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'BAKU, REPUBLIC OF AZERBAIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'ADDIS, ETHIOPIA': 'ADDIS ABABA, ETHIOPIA',\n",
    "    'ANKEY, IA': 'ANKENY, IA'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and so on. (This is tedious work, and -- again -- tools like OpenRefine make this process somewhat less tedious. But if you have a long-term project that involves data that will be updated regularly, and it's worth putting in the time to make sure the data are cleaned the same way each time, you can do it all in pandas.)\n",
    "\n",
    "Here's how we might _apply_ our bulk find-and-replace dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replace_destination(row):\n",
    "    '''Given a row of data, see if the value is a typo to be replaced'''\n",
    "    \n",
    "    # get the clean destination value\n",
    "    dest = row['destination_clean']\n",
    "    \n",
    "    # try to look it up in the `typo_fixes` dictionary\n",
    "    # the `get()` method will return None if it doesn't find a match\n",
    "    typo = typo_fixes.get(dest)\n",
    "    \n",
    "    # then we can test to see if `get()` got an item out of the dictionary (True)\n",
    "    # or if it returned None (False)\n",
    "    if typo:\n",
    "        # if it found an entry in our dictionary,\n",
    "        # return the value from that key/value pair\n",
    "        return typo_fixes[dest]\n",
    "    # otherwise\n",
    "    else:\n",
    "        # return the original destination string\n",
    "        return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function and overwrite our working \"clean' column\"\n",
    "df_junkets['destination_clean'] = df_junkets.apply(find_replace_destination, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "This just scratches the surface of what you can do in pandas. Here are some other resources to check out:\n",
    "\n",
    "- [Pythonic Data Cleaning With NumPy and Pandas](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
    "- [pandas official list of tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "- [Karrie Kehoe's guide to cleaning data in pandas](https://github.com/KarrieK/pandas_data_cleaning)\n",
    "- [Data cleaning with Python](https://www.dataquest.io/blog/data-cleaning-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
