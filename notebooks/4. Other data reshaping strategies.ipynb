{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conceptual-asthma",
   "metadata": {},
   "source": [
    "# 4. Other data reshaping strategies\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "- The `pivot_table()` method\n",
    "- Filling null values\n",
    "- Melting wide data to long data\n",
    "- Building one dataframe out of a directory of files formatted the same way\n",
    "\n",
    "First, let's import pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-miracle",
   "metadata": {},
   "source": [
    "### `pivot_table()`\n",
    "\n",
    "Sometimes, when you group and aggregate, the `groupby()` method doesn't _quite_ give you the view you need of your data, and the [`pivot_table()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) method can be a more intuitive choice.\n",
    "\n",
    "For this section, we'll be examining a dataset of foreign eel product imported to the U.S. ([source](https://www.fisheries.noaa.gov/national/sustainable-fisheries/foreign-fishery-trade-data)), which you will find in `../data/eels.csv`. Every row is one month's worth of eel product imports to the U.S. from one country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eels = pd.read_csv('../data/eels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-center",
   "metadata": {},
   "source": [
    "Let's take a look at import volume by country by year. If we were making this pivot table in Excel, we would drag `country` to Rows, `kilos` to Values and `year` to Columns. But we're gonna do it in pandas! We need to hand the `pivot_table()` method four things:\n",
    "- The data frame you're pivoting (`df_eels`)\n",
    "- The `index` column -- what to group your data by (`index='country'`)\n",
    "- The `columns` column -- the second grouping factor (`columns='year'`)\n",
    "- The `values` column -- what column are we doing math on? (`values='kilos'`)\n",
    "- The `aggfunc` -- what function to use to aggregate the data; the default is to use an average, but we'll use Python's built-in `sum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-ordinary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eels_pivoted = pd.pivot_table(df_eels,\n",
    "                              index='country',\n",
    "                              columns='year',\n",
    "                              values='kilos',\n",
    "                              aggfunc=sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "eels_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-detroit",
   "metadata": {},
   "source": [
    "### Filling null values\n",
    "\n",
    "Right now, if a country didn't have any shipments of these eel products in a given year, the values are `NaN`. To fill those values with zeroes instead, which might make more sense in context, you can use the [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "eels_pivoted_clean = eels_pivoted.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "eels_pivoted_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-college",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "In the cells below:\n",
    "- Sort the `eels_pivoted_clean` dataframe descending on the `2017` column\n",
    "- Show the sum of all eel imports from 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-leone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-heaven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minimal-maria",
   "metadata": {},
   "source": [
    "### Melt wide data to long\n",
    "\n",
    "Sometimes, you have data that's hard to work with because it's [wide instead of long](https://en.wikipedia.org/wiki/Wide_and_narrow_data). You can use the pandas [`melt()`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) method to \"un-pivot\" wide data into tidy data.\n",
    "\n",
    "For this example, we're going to load in a CSV of African doctor emigration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors = pd.read_csv('../data/africa-physician-emigration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-shirt",
   "metadata": {},
   "source": [
    "This data would be much easier to work with if every row were the sending country, the receiving country and the number of doctors. According to [the documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html), we need to hand this method a couple of keyword arguments:\n",
    "- `id_vars`: The column, or column, that will be the \"identifying variable\" that won't change -- in this case, `Sending country`\n",
    "- `value_vars`: A column name, or a list of column names, with the values we want to melt -- in this case, everything but the identifying column (more on this in a minute)\n",
    "- `var_name`: What should we call the resulting column with the variable names in it? In this case, `Receiving country`\n",
    "- `value_name`: What should we call the resulting column with the values in it? In this case, `Number of doctors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors.melt(id_vars='Sending country',\n",
    "                value_vars=['UK', 'USA', 'France', 'Canada', 'Australia', 'Portugal', 'Spain', 'Belgium', 'So. Africa'],\n",
    "                var_name='Receiving country',\n",
    "                value_name='Number of doctors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-organ",
   "metadata": {},
   "source": [
    "One thing -- having to list out the names of all the columns for the `value_vars` keyword argument was kind of tedious, so here's a more efficient way to do that: by accessing the `columns` attribute of the dataframe and using list indexing to grab just the ones we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we get if we leave off the first one?\n",
    "df_doctors.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-spirit",
   "metadata": {},
   "source": [
    "Bingo! Let's try that again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors.melt(id_vars='Sending country',\n",
    "                value_vars=df_doctors.columns[1:],\n",
    "                var_name='Receiving country',\n",
    "                value_name='Number of doctors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-viking",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "In the cells below:\n",
    "- Re-run the code from the cell above but assign the new dataframe to a variable\n",
    "- Filter that dataframe to show only records where the receiving country is the USA\n",
    "- Filter that dataframe to show only records where the number of physicians is greater than 100\n",
    "- Sort the data descending by the number of doctors emigrating -- what's the largest outflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-utilization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-laptop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-indiana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-channels",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "literary-tumor",
   "metadata": {},
   "source": [
    "### Building a dataframe from a directory of files\n",
    "\n",
    "Sometimes, rather than one file you need to load, you have a directory of files with the same format but different data. Let's talk about a strategy for reading them all into a single dataframe -- the data for this exercise comes from [this wonderful data-driven story from 2019 by C.K. Hickey in _Foreign Policy_](https://foreignpolicy.com/all-the-presidents-meals-state-dinners-white-house-infographic/) on state dinner menus for U.S. presidents (thank you, C.K.!) and can be found in the `../data/state-dinners/` directory.\n",
    "\n",
    "Our strategy:\n",
    "- Get a list of these files using [the `glob` module](https://docs.python.org/3/library/glob.html) from the standard library\n",
    "- Use a fun Python data structure called a [\"list comprehension\"](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) in conjunction with the pandas methods `read_csv()` (which we've seen before) and [`concat()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) (which we have not)\n",
    "\n",
    "First, we need to import `glob` before we can use it. (FWIW: The customary thing to do is drop all your imports at the top of your script.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-referral",
   "metadata": {},
   "source": [
    "Get a list of the files using wildcards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_files = glob.glob('../data/state-dinners/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-sigma",
   "metadata": {},
   "source": [
    "In human language: Go to the `glob` module we just imported and use its `glob` object to get a list of files based on the path and filename wildcards we hand it.\n",
    "\n",
    "Now let's talk for a sec about list comprehensions. Let's say you had a list of items that you wanted to _do_ something to -- some math, some filtering, some reading into dataframes. (We're about to do this last one!) One of the main uses for list comprehensions is effeciently \"saving\" the results of this operation to a new variable.\n",
    "\n",
    "Here's a simple example -- let's say we had the following list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-recording",
   "metadata": {},
   "source": [
    "... and we want to end up with a list of numbers that is each of these numbers multiplied by 10. We could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for x in number_list:\n",
    "    new_list.append(x*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-order",
   "metadata": {},
   "source": [
    "You could achieve the same thing with a _list comprehension_ much quicker and easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_lc = [x*10 for x in number_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-religious",
   "metadata": {},
   "source": [
    "Here, `x` is a placeholder for each item in the list, same as the variable defined in the `for` loop.\n",
    "\n",
    "That's basically what we're going to do here -- instead of creating an empty list, looping over each file in the `state_dinners` directory, creating a new dataframe, adding it to the list, then concatenating all those dataframes, we can do it all in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dinners = pd.concat([pd.read_csv(x) for x in sd_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-custody",
   "metadata": {},
   "source": [
    "Reading this from the inside out as a human sentence: Take each CSV file in the `state_dinners` directory, which we found earlier using the `glob` tool, and read it into a (more or less temporary) dataframe -- then take all of those dataframes and concatenate them together into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dinners.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
